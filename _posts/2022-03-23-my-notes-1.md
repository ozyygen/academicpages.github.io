---
title: 'Knowledge Graph Embeddings and Link Prediction For Beginners'
date: 2022-03-23
permalink: /posts/2022/03/my-notes-1/
tags:
  - knowledge graph
  - kgembeddings
  - knowledge graph embeddings
  - embeddings introduction
---

 Did you ever think about representing an object as vectors? Hopefully you will have an understanding on this matter after reading the post!

Wait, Wait! What is a Knowledge Graph?
======

Knowledge Graphs, shortly KGs, store information about things and also store how these things related to each other with its extendable graph tree-like structure. In reality, you can see most of the things - it can be anything a person, a book, an abstract term, etc. - are related to each other naturally. It might be a good practice to use KG if the information is mainly linked or related to the rest. 
KGs are simple: To make it more precise just imagine storing your object information - they can be abstract such as time or concrete such as a person - in a small circle that connected with other circles that stores other objects. Connections between objects can be lengthy and difficult to track. The triple structure can be seen as the solution for this long, difficult-to-track chain. We can tackle two objects and their relationships in a time to make the interpretation easier. In this logic, two related objects and their relationships are considered as triple and this is the essential building block that creates the KG structure. We can show the information about Alice who plays violin as a triple with the figure below:

---
<div style="text-align: center;">
  <img src='/images/triple.png'>
</div>
---

Predictions, Recommendations, and more? You are at the right place!
======

Okay, now we know KGs store linked information, but is the information that we do not understand really knowledge?  - To be honest, I didn't plan that the question would sound so philosophical - It should not be difficult at all to infer or even make future possible predictions from a very small number of related objects or things. The human can understand relations between things to a certain extent, but what if there is a structure containing thousands or millions of things? When it comes to extended understanding, machine learning comes into play. 

One way to mine and derive implicit information from KGs is using Knowledge Graph Embedding (KGE) techniques. There are different graph embedding techniques in the field for satisfying different requirements such as time-aware embedding models for the situations where time information is in consideration. Embedding methods have a very interesting way to represent things to analyze them: As a vector in a low-dimensional space. In this approach, an embedding model processes knowledge graph entities and relations to represent them as vectors in a two-dimensional space. We are using dimensional spaces to define points, vectors, etc. with their coordinates. For instance, a three-dimensional space has three different dimensions x, y, and z. 

In general, KGEs train their model for locating - or mapping - entities and relations in a low-dimensional space. After training is completed, located entities and relations give insight about how similar they are according to their distance in the embedding space. The distance can be calculated in various ways. Let's assume a situation where there is a candidate triple whose probable accuracy is desired to be calculated with TransE. It uses L1 or L2 distance for (the learned embedding of head + the learned embedding of relation) and the learned embedding of tail vectors of a triple. If the calculated distance is less than a threshold, this considered triple can be qualified as a positive triple otherwise that triple is negative or false.

Let's explain step by step how KGE works from training to prediction:
- The dataset that has the information to analyze and mine might require to be manipulated to better KGE results. For instance, cleaning blank rows, enriching data with relevant knowledge, etc. Also, you can split a dataset into train, test, and validation sets to see the quality of the learned embedding after training. 
- To run KGE methods with the input data, it's easy to use one of the existing approaches, such as DGL-KE, that allow you to train your data with a selection of KGE models and adjustable machine learning metrics on its framework.
- We can split the KG into the train, test, and validation sets. The training set is required to train the embedding model at the training step of embeddings, and test and validation sets are for to evaluate the model performance to see the quality of the resulting embedding model with the ranking metrics (such as MRR and Hit@10).
- For the prediction, based on the trained model and distance measurements, KGEs calculate scores to rank possible new entities and relation associations and provide triples and their prediction scores as the result.



See you in other posts. Stay with science!
------